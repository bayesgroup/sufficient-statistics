<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " vocab="http://ogp.me/ns" lang="ru">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Self-Normalizing Neural Networks | Bayesian Summaries</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/lanyon.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="canonical" href="http://bayesgroup.github.io/sufficient-statistics/posts/self-normalizing-neural-networks/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Артём Соболев">
<link rel="next" href="../improved-variational-autoencoders-for-text-modeling-using-dilated-convolutions/" title="Improved Variational Autoencoders for Text Modeling using Dilated Convolutions" type="text/html">
<meta property="og:site_name" content="Bayesian Summaries">
<meta property="og:title" content="Self-Normalizing Neural Networks">
<meta property="og:url" content="http://bayesgroup.github.io/sufficient-statistics/posts/self-normalizing-neural-networks/">
<meta property="og:description" content="Мотивация
У по-настоящему глубоких нейросетей очень много слоёв с нелинейностями, которые либо раздувают входные параметры, либо скукоживают их, а хотелось бы, чтобы распределение выходов не менялось ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-06-18T18:07:14+03:00">
<meta property="article:tag" content="activations">
<meta property="article:tag" content="batch normalization">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="exploding gradients">
<meta property="article:tag" content="mathjax">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <div class="sidebar-item">
            <p>A reserved <a href="https://getnikola.com" target="_blank">Nikola</a> theme that places the utmost gravity on content with a hidden drawer. Made by <a href="https://twitter.com/mdo" target="_blank">@mdo</a> for Jekyll,
            ported to Nikola by <a href="https://twitter.com/ralsina" target="_blank">@ralsina</a>.</p>
        </div>
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../archive.html">Архив</a>
        <a class="sidebar-nav-item" href="../../categories/">Тэги</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="../../" title="Bayesian Summaries" rel="home">Bayesian Summaries</a>
    </h3>

        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><div class="paper-link">
        <a href="https://arxiv.org/abs/1706.02515">Paper</a>
    </div>
    <header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">Self-Normalizing Neural Networks</a></h1>


    
        <ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/activations/" rel="tag">activations</a></li>
            <li><a class="tag p-category" href="../../categories/batch-normalization/" rel="tag">batch normalization</a></li>
            <li><a class="tag p-category" href="../../categories/deep-learning/" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../categories/exploding-gradients/" rel="tag">exploding gradients</a></li>
        </ul></header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<h2>Мотивация</h2>
<p>У по-настоящему глубоких нейросетей очень много слоёв с нелинейностями, которые либо раздувают входные параметры, либо скукоживают их, а хотелось бы, чтобы распределение выходов не менялось – тем самым мы достигнем эффекта нормализации, что потенциально позволит нам выпилить уже батчнорм наконец (который многим не нравится, например, из-за внесения зависимости между семплами батча, см. <a href="http://www.alexirpan.com/2017/04/26/perils-batch-norm.html">сюда</a>)</p>
<h2>Как это делать?</h2>
<p>По-сути предлагается рассмотреть параметрическую функцию активации, а потом найти такие параметры, которые будут высокодисперсные выходы сужать, а малодисперсные – расширять. Вообще, от функции активации мы хотим:</p>
<ol>
<li>Как отрицательные, так и положительные значения (Иначе она будет смещать распределение активаций, которое придётся компенсировать весами).</li>
<li>Область насыщения (производная близка к нулю) для тех самых высокодисперсных входов.</li>
<li>Область возрастания с наклоном &gt; 1 для раздутия малодисперсных выходов.</li>
<li>Непрерывность – тем самым гарантируется, что есть неподвижная точка, уравновешивающая два предыдущих пункта.</li>
</ol>
<p>Обычная ELU выглядит так: $\text{ELU}(x) = x [x &gt; 0] + \alpha (\exp(x) - 1) [x &lt; 0]$ ($\alpha$ – положительный гиперпараметр). Нетрудно заметить, что функция не удовлетворяет только п. 3, поэтому мы это пофиксим домножением всей функции на параметр масштаба $\lambda$, получим тем самым Scaled ELU (SELU): $\text{SELU}(x) = \lambda \text{ELU}(x)$. Однако, выбирать гиперпараметры $\alpha$ и $\lambda$ от балды – затея сомнительная, поэтому давайте попробуем подобрать их так, чтобы распределение нейронов между слоями менялось несильно, а именно, чтобы мат. ожидание и дисперсия выходов соседних (и, следственно, всех) слоёв не менялись.</p>
<h2>Математика</h2>
<p>Мы предполагаем, что</p>
<ul>
<li>Входы нейросети (aka выходы воображаемого нулевого слоя) являются независимыми одинаково распределёнными величинами со средним 0 и дисперсией 1 (не такое уж необоснованное предположение, всегда можно сделать whitening на стадии предобработки)</li>
<li>Предполагается, что веса нейросети такие, что сумма весов, входящих в один нейрон, примерно 0, а сумма их квадратов примерно 1 (как если бы они были независимыми случайными величинами со средним 0 и дисперсией 1/n). Лично мне это предположение кажется существенно менее обоснованным</li>
<li>Посмотрим на преактивации на $k+1$-ом слое: это взвешенная сумма выходов предыдущего слоя, т.е. сумма кучи (у нас же широкие сети, да?) случайных величин. Что делает диплёрнер, когда видит сумму непонятно как связанных случайных величин? Правильно, скрещивает пальцы и кастует ЦПТ. То есть, мы верим, что преактивации распределены нормально (для дополнительного успокоения можно бубнить себе про CLT under weak dependence).</li>
</ul>
<p>Все эти махинации теперь позволяют нам найти мат. ожидание и дисперсию самих активаций, посчитав пару гадких интегралов (а именно первые 2 момента SELU-преобразованной нормальной случайной величины). Получаем мешанину из erf, которая зависит от искомых гиперпараметров. Мы приравниваем эту мешанину к тем средним и дисперсии, что были на выходе предыдущего слоя (т.е. 0 и 1, мы ведь договорились, что входы будут такие, но формулы можно пересчитать и для других вариантов). Разумеется, аналитически это никто решать не собирается – авторы в ноутбуке находят правильные $\alpha$ и $\lambda$ с помощью sympy (хотя могли бы и ручками посчитать, там по $\alpha$ получается линейное уравнение, а по $\lambda$ – квадратичное).</p>
<p>Т.е. если наши предположения выполнены, можно найти гиперпараметры, для которых случайные величины с нулевым средним и единичной дисперсией будут неподвижной точкой в смысле сохранения этих моментов. Однако, ещё полезно показать, что неподвижная точка стабильная и притягивающая, ведь наши предположения всегда выполнены приближённо. Потом ещё показывается, что предположения о нормированности весов тоже необязательно выполнять точно.</p>
<p>Важно заметить, что веса сети следует инициализировать в соответствие с предположениями метода, т.е. из распределения с нулевым средним и дисперсией $1/n$ ($n$ – количество нейронов в предыдущем слое).
А ещё там есть параграф про то, как модифицировать дропаут так, чтобы он хорошо работал с предложенным методом.</p>
<h2>Эксперименты</h2>
<p>Все эксперименты (как и идея статьи) были произведены на обычных полносвязных нейросетях, никаких свёрток или RNN'ок (об этом позже). Один из экспериментов – 121 классификационный датасет из UCI Machine Learning Repository, где SNN победили все методы (включая классические SVM и RF, про xgboost ни слова) на датасетах от 1000 датапоинтов и уступили этим классическим методам на меньших датасетах. А ещё в ходе подбора гиперпараметров SNN'ы оказались типично глубже других нейросетей. И ещё два не очень больших датасета, где метод тоже показал себя хорошо.</p>
<p>В общем, авторы побили другие архитектуры (стандартные и не очень) со всякими нормализациями и без.</p>
<h2>Extra</h2>
<ul>
<li>Авторы заявляют, что (отмасштабированная) ReLU не даст такого самонормализующего эффекта. Однако, если её ещё и сдвинуть немного ($\text{SReLU}(x) = \lambda (\max(0, x + \alpha) - \alpha)$), то все требования начинают выполняться (правда, такая активация негладкая в отличие от SELU, что может сломать какие-нибудь теоремы), говорят, кто-то на просторах твиттера так сделал</li>
<li>В статье нет ни слова про другие архитектуры, что, учитывая то, что LSTM были придуманы последним автором, намекает на то, что на RNN'ках этот подход не работает (хоть авторы и отнекиваются, говоря, что они "не проводили эксперименты")</li>
<li>В статье ничего не говорится про bias'ы, формулы написаны так, как будто их нет (вовремя я, да?). Плохие новости для нулевых входов, ведь $\text{SELU}(0) = 0$</li>
</ul>
<h2>Резюме</h2>
<p>Идея интересная, результаты игрушечные (хоть и убедительно показывают, что для обычных задач с глубокими полносвязными сетями SELU помогает), предположения сомнительные (что может быть причиной отсутствия экспериментов на других аврхитектурах в статье, да и в инетрнете люди рапортуют переменный успех новой активации на интересных задачах вроде DRL).</p>
</div>
    </div>
    <aside class="postpromonav"><div class="post-meta">
    
        <span class="post-author">Артём Соболев</span>
    <time class="post-date" datetime="2017-06-18T18:07:14+03:00" title="18 июня 2017">18 июня 2017</time>
</div>
    </aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><footer id="footer"><p>Contents © 2017         <a href="mailto:info@bayesgroup.ru">BayesGroup</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
    
            <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><!-- fancy dates --><script>
    moment.locale("ru");
    fancydates(2, "DD.MM.YYYY");
    </script><!-- end fancy dates -->
</body>
</html>
