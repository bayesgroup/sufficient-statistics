title: Self-Normalizing Neural Networks
slug: self-normalizing-neural-networks
date: 2017-06-18 18:07:14 UTC+03:00
author: Артём Соболев
link: https://arxiv.org/abs/1706.02515
tags: mathjax, deep learning, batch normalization, exploding gradients, activations

# Мотивация
У по-настоящему глубоких нейросетей очень много слоёв с нелинейностями, которые либо раздувают входные параметры, либо скукоживают их, а хотелось бы, чтобы распределение выходов не менялось – тем самым мы достигнем эффекта нормализации, что потенциально позволит нам выпилить уже батчнорм наконец (который многим не нравится, например, из-за внесения зависимости между семплами батча, см. [сюда](http://www.alexirpan.com/2017/04/26/perils-batch-norm.html))

# Как это делать?
По-сути предлагается рассмотреть параметрическую функцию активации, а потом найти такие параметры, которые будут высокодисперсные выходы сужать, а малодисперсные – расширять. Вообще, от функции активации мы хотим:

1. Как отрицательные, так и положительные значения (Иначе она будет смещать распределение активаций, которое придётся компенсировать весами).
2. Область насыщения (производная близка к нулю) для тех самых высокодисперсных входов.
3. Область возрастания с наклоном > 1 для раздутия малодисперсных выходов.
4. Непрерывность – тем самым гарантируется, что есть неподвижная точка, уравновешивающая два предыдущих пункта.

Обычная ELU выглядит так: $\text{ELU}(x) = x [x > 0] + \alpha (\exp(x) - 1) [x < 0]$ ($\alpha$ – положительный гиперпараметр). Нетрудно заметить, что функция не удовлетворяет только п. 3, поэтому мы это пофиксим домножением всей функции на параметр масштаба $\lambda$, получим тем самым Scaled ELU (SELU): $\text{SELU}(x) = \lambda \text{ELU}(x)$. Однако, выбирать гиперпараметры $\alpha$ и $\lambda$ от балды – затея сомнительная, поэтому давайте попробуем подобрать их так, чтобы распределение нейронов между слоями менялось несильно, а именно, чтобы мат. ожидание и дисперсия выходов соседних (и, следственно, всех) слоёв не менялись.

# Математика
Мы предполагаем, что

* Входы нейросети (aka выходы воображаемого нулевого слоя) являются независимыми одинаково распределёнными величинами со средним 0 и дисперсией 1 (не такое уж необоснованное предположение, всегда можно сделать whitening на стадии предобработки)
* Предполагается, что веса нейросети такие, что сумма весов, входящих в один нейрон, примерно 0, а сумма их квадратов примерно 1 (как если бы они были независимыми случайными величинами со средним 0 и дисперсией 1/n). Лично мне это предположение кажется существенно менее обоснованным
* Посмотрим на преактивации на $k+1$-ом слое: это взвешенная сумма выходов предыдущего слоя, т.е. сумма кучи (у нас же широкие сети, да?) случайных величин. Что делает диплёрнер, когда видит сумму непонятно как связанных случайных величин? Правильно, скрещивает пальцы и кастует ЦПТ. То есть, мы верим, что преактивации распределены нормально (для дополнительного успокоения можно бубнить себе про CLT under weak dependence).

Все эти махинации теперь позволяют нам найти мат. ожидание и дисперсию самих активаций, посчитав пару гадких интегралов (а именно первые 2 момента SELU-преобразованной нормальной случайной величины). Получаем мешанину из erf, которая зависит от искомых гиперпараметров. Мы приравниваем эту мешанину к тем средним и дисперсии, что были на выходе предыдущего слоя (т.е. 0 и 1, мы ведь договорились, что входы будут такие, но формулы можно пересчитать и для других вариантов). Разумеется, аналитически это никто решать не собирается – авторы в ноутбуке находят правильные $\alpha$ и $\lambda$ с помощью sympy (хотя могли бы и ручками посчитать, там по $\alpha$ получается линейное уравнение, а по $\lambda$ – квадратичное).

Т.е. если наши предположения выполнены, можно найти гиперпараметры, для которых случайные величины с нулевым средним и единичной дисперсией будут неподвижной точкой в смысле сохранения этих моментов. Однако, ещё полезно показать, что неподвижная точка стабильная и притягивающая, ведь наши предположения всегда выполнены приближённо. Потом ещё показывается, что предположения о нормированности весов тоже необязательно выполнять точно.

Важно заметить, что веса сети следует инициализировать в соответствие с предположениями метода, т.е. из распределения с нулевым средним и дисперсией $1/n$ ($n$ – количество нейронов в предыдущем слое).
А ещё там есть параграф про то, как модифицировать дропаут так, чтобы он хорошо работал с предложенным методом.

# Эксперименты

Все эксперименты (как и идея статьи) были произведены на обычных полносвязных нейросетях, никаких свёрток или RNN'ок (об этом позже). Один из экспериментов – 121 классификационный датасет из UCI Machine Learning Repository, где SNN победили все методы (включая классические SVM и RF, про xgboost ни слова) на датасетах от 1000 датапоинтов и уступили этим классическим методам на меньших датасетах. А ещё в ходе подбора гиперпараметров SNN'ы оказались типично глубже других нейросетей. И ещё два не очень больших датасета, где метод тоже показал себя хорошо.

В общем, авторы побили другие архитектуры (стандартные и не очень) со всякими нормализациями и без.

# Extra

* Авторы заявляют, что (отмасштабированная) ReLU не даст такого самонормализующего эффекта. Однако, если её ещё и сдвинуть немного ($\text{SReLU}(x) = \lambda (\max(0, x + \alpha) - \alpha)$), то все требования начинают выполняться (правда, такая активация негладкая в отличие от SELU, что может сломать какие-нибудь теоремы), говорят, кто-то на просторах твиттера так сделал
* В статье нет ни слова про другие архитектуры, что, учитывая то, что LSTM были придуманы последним автором, намекает на то, что на RNN'ках этот подход не работает (хоть авторы и отнекиваются, говоря, что они "не проводили эксперименты")
* В статье ничего не говорится про bias'ы, формулы написаны так, как будто их нет (вовремя я, да?). Плохие новости для нулевых входов, ведь $\text{SELU}(0) = 0$

# Резюме
Идея интересная, результаты игрушечные (хоть и убедительно показывают, что для обычных задач с глубокими полносвязными сетями SELU помогает), предположения сомнительные (что может быть причиной отсутствия экспериментов на других аврхитектурах в статье, да и в инетрнете люди рапортуют переменный успех новой активации на интересных задачах вроде DRL).
