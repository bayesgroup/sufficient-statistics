<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " vocab="http://ogp.me/ns" lang="ru">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Breaking the Softmax Bottleneck: A High-Rank RNN Language Model | Codename 'Sufficient Statistics'</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/lanyon.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="canonical" href="http://bayesgroup.github.io/sufficient-statistics/posts/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Артём Соболев">
<link rel="prev" href="../hierarchical-implicit-models-and-likelihood-free-variational-inference/" title="Hierarchical Implicit Models and Likelihood-Free Variational Inference" type="text/html">
<link rel="next" href="../kernel-implicit-variational-inference/" title="Kernel Implicit Variational Inference" type="text/html">
<meta property="og:site_name" content="Codename 'Sufficient Statistics'">
<meta property="og:title" content="Breaking the Softmax Bottleneck: A High-Rank RNN Language Model">
<meta property="og:url" content="http://bayesgroup.github.io/sufficient-statistics/posts/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/">
<meta property="og:description" content="Довольно интересная статья о том, что простого софтмакса на выходе языковой модели недостаточно для достаточнго адекватного представления языка.
Проблема
В задаче изучения языковой модели можно усмотр">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-11-14T00:33:03+03:00">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="text modeling">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <div class="sidebar-item">
            <p>Обзоры не являются официальной позицией группы и предназначены исключительно для внутреннего использования.</p>
        </div>
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../authors/">Авторы</a>
        <a class="sidebar-nav-item" href="../../archive.html">Архив</a>
        <a class="sidebar-nav-item" href="../../categories/">Тэги</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="../../" title="Codename 'Sufficient Statistics'" rel="home">Codename 'Sufficient Statistics'</a>
    </h3>


        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><div class="paper-link">
        <a href="https://arxiv.org/abs/1711.03953">Paper</a>
    </div>
    <header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</a></h1>

        
        <ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/deep-learning/" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../categories/text-modeling/" rel="tag">text modeling</a></li>
        </ul></header><div class="e-content entry-content" itemprop="articleBody text">
    <p>Довольно интересная статья о том, что простого софтмакса на выходе языковой модели недостаточно для достаточнго адекватного представления языка.</p>
<h1 id="проблема">Проблема</h1>
<p>В задаче изучения языковой модели можно усмотреть задачу матричного разложения. А именно, пусть мы моделируем <span class="math inline">\(p(X|c)\)</span> где <span class="math inline">\(X \in \mathcal{D}\)</span> – токен из словаря размера <span class="math inline">\(M\)</span>, а <span class="math inline">\(c\)</span> – некоторый контекст (допустим, что множество возможных контекстов конечно<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> и имеет мощность <span class="math inline">\(N\)</span>). Так же заметим, что любая языковая модель может быть определена следующей матрицей (обозначим за <span class="math inline">\(p^*(X|c)\)</span> генеративную модель языка, из которой мы наблюдаем семплы):</p>
<p><span class="math display">\[
A = \left(
    \begin{array}{ccc}
    \log p^*(x_1|c_1) &amp; \dots &amp; \log p^*(x_1|c_N) \\
     &amp; \ddots &amp; \\
    \log p^*(x_M|c_1) &amp; \dots &amp; \log p^*(x_M|c_N) \\
    \end{array}
    \right)
\]</span></p>
<p>Обозначим за <span class="math inline">\(W \in \mathbb{R}^{M \times D}\)</span> матрицу эмбеддингов, т.е. такую матрицу в строке <span class="math inline">\(m\)</span> которой записано векторное представление слова <span class="math inline">\(x_m\)</span>. За <span class="math inline">\(H \in \mathbb{R}^{N \times D}\)</span> обозначим матрицу эмбеддингов контекстов, где в строке <span class="math inline">\(n\)</span> записано векторное представление контекста <span class="math inline">\(c_n\)</span>. Тогда можно показать, что если мы пытаемся моделировать языковую модель с помощью софтмакса <span class="math inline">\(p(X|c) = \text{Softmax}(W H^T)\)</span>, то это эквивалентно решению задачи факторизации <span class="math display">\[ W H^T = A' \]</span> где <span class="math inline">\(A'\)</span> равно <span class="math inline">\(A\)</span> с точностью до добавления произвольных констант к строкам (всем элементам, то есть) матрицы.</p>
<p>Возможна ли такая факторизация? Утверждается, что операция добавления произвольных констант к строкам матрицы <span class="math inline">\(A \mapsto A'\)</span> изменяет ранг матрицы не более чем на 1. Произведение <span class="math inline">\(W H^T\)</span> в лучшем случае, очевидно, имеет ранг <span class="math inline">\(D\)</span>. То есть, чтобы выучить произвольно сложную языковую модель, нужно иметь векторные представления размерности как минимум <span class="math inline">\(\text{rank}(A)-1\)</span>. Авторы называют это утверждение бутылочным горлышком софтмакса (<strong>Softmax Bottleneck</strong>)</p>
<h2 id="насколько-велик-ранг-естественного-языка">Насколько велик ранг Естественного Языка?</h2>
<p>К сожалению, мы не можем посчитать ранг естественного языка<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, поэтому в соответствующей части авторы ограничиваются спекуляциями:</p>
<ol type="1">
<li>Кажется, что естественный язык очень сильно контекстно зависим. Авторы приводят пример: мы ожидаем, что за словом “Северная” будет следовать “Корея”, если речь идёт о полотической колонке, и не ожидаем такого в тексте об американской истории.</li>
<li>Если бы ранк языка был невелик, людям было бы достаточно пары сотен базовых пар контекст-токен. Однако, кажется, это не так.</li>
<li>Эмпирически, чем выше ранк, тем лучше.</li>
</ol>
<h2 id="окей-ранг-большой-но-может-можно-сделать-что-то-простое">Окей, ранг большой, но может можно сделать что-то простое?</h2>
<p>Авторы рассматривают 2 варианта: n-грамное представление и повышение размерности. Оба непрактичны: n-грамное представление экспоненциально быстро разрастается в размере, повышение размера векторного представления до размера словаря тоже плохо масштабируется.</p>
<h1 id="смесь-софтмаксов">Смесь Софтмаксов</h1>
<p>Предлагается для каждой пары контекст-токен <span class="math inline">\((c, x)\)</span> генерировать <span class="math inline">\(K\)</span><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> софтмаксов, а потом смешивать их с весами <span class="math inline">\(\pi_c\)</span>:</p>
<p><span class="math display">\[
p(x|c) = \sum_{k=1}^K \pi_{c,k} \text{Softmax}(h^T_{c,k} w_x),
\quad\quad
\sum_{k=1}^K \pi_{c,k} = 1
\]</span></p>
<p>В реккурентках это предлагается применять так: <span class="math inline">\(\pi_c\)</span> получается из скрытого состояния с помощью дополнительного полносвязного слоя с софтмаксом на выходе, <span class="math inline">\(h_{c,k}\)</span> получаются полносвязным слоем с какой-нибудь обычной нелинейностью (в статье авторы используют <span class="math inline">\(\tanh\)</span>), из которых потом с помощью полносвязного слоя (который неявно выучивает эмбеддинги токенов) генерируются софтмаксы для смешивания.</p>
<p>В чём идея этой, казалось бы, бесмысленной операции? Как я понимаю, хочется, чтобы <span class="math inline">\(A\)</span> перестала быть линейной по эмбеддингам. Тогда можно воспользоваться всеми плюшкам нелинейностей, благодаря которым работают нейросети<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
<p>Далее авторы говорят, что если вы вдруг смешаете логиты, а не софтмаксы, то ничего путного из этого не выйдет, но это и так очевидно, мы же тогда получим обычные эмбеддинги.</p>
<h1 id="эксперименты">Эксперименты</h1>
<p>Language Modeling авторы оценивали по перплексии на Peтn Treebank (PTB):</p>
<p><img src="../../post-images/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/table1.png"></p>
<p>И на WikiText-2 (WT2):</p>
<p><img src="../../post-images/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/table2.png"></p>
<p>В доказательство общности предлагаемого подхода авторы предлагают результаты для моделирования диалогов с помощью seq2seq’а, где выходной softmax заменён на предлагаемый ими вариант. Здесь MoS – Mixture of Softmaxes (предлагаемый вариант), а MoC – Mixture of Contexts – как если бы мы вдруг смешали логиты (это, как мы уже знаем, проблему не решает)</p>
<p><img src="../../post-images/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/table3.png"></p>
<h2 id="ablation-study">Ablation study</h2>
<p>Чтобы удостовериться, что это именно смесь софтмаксов помогает, авторы всячески сравнивают смесь софтмаксов (MoS) со смесью контекстов (MoC) с одинаковым количеством параметров. Смесь контекстов на удивление работает лучше обычного софтмакса, но только на PTB. Тем не менее, смесь софтмаксов стабильно лучше.</p>
<p><img src="../../post-images/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/table4.png"></p>
<h2 id="анализ-рангов">Анализ рангов</h2>
<p>Давайте построим матрицы, подобные <span class="math inline">\(A\)</span> для софтмакса, смеси контекстов и смеси софтмаксов (для реального датасета, конечно, построить её сложно, мы ведь не наблюдаем всех элементов матрицы). Кроме того, авторы предполагают, что модели, хорошо захватывающие контекст, будут иметь высокую KL дивергенцию для распределений на следующий токен при условии разных контекстов <span class="math inline">\(D_{KL}(p(X|c)||p(X|c'))\)</span>. Соответственно, предлагается случайно насемплировать сколько-то пар контекстов и посчитать среднюю KL дивергенцию между соответствующими условными распределениями.</p>
<p><img src="../../post-images/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/table5.png" class="medium"></p>
<p>Видно, что и ранг у предложенной модели сильно выше, и средняя KL дивергенция.</p>
<p>А ещё авторы проанализировали CDF отнормированных сингулярных чисел получающихся матриц. Видно, что и у софтмакса, и у смеси контекстов большинство сигнулярных чисел довольно мало, а вот у смеси софтмаксов почти все с.ч. больше.</p>
<p><img src="../../post-images/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/figure1.png"></p>
<h2 id="анализ-семплов">Анализ семплов</h2>
<p>Наконец, авторы посмотрели на семплы (полученные бимсёрчем на PTB) смеси контекстов (раз она работает чуть лучше софтмакса) и смеси контекста и попытались сделать далеко идущие выводы. Я их копировать не буду, но можете сами посмотреть на семплы:</p>
<p><img src="../../post-images/breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model/table6.png"></p>
<h1 id="резюме">Резюме</h1>
<p>Довольно интересная работа, показывающая, что сколько бы совершенную RNN’ку вы ни имели, всегда будет бутылочное горлышко в виде размерности векторного представления токена и контекста. В этой статье предлагается, по-сути, нелинейное (ну, существенно более нелинейное, чем нормированная экспонента скалярного произведения) преобразование этих векторных представлений, что позволяет повысить выразительную силу получающейся вероятностной модели. <a href="https://github.com/zihangdai/mos">Код</a> доступен на гитхабе.</p>
<p>Вместе с тем, неочевидно, какой эффект это всё оказывает на время выполнения. Кажется, что последний слой с софтмаксами очень толстый и считать его много раз не очень приятно. С другой стороны, возможно такой подход позволит пользоваться эмбеддингами меньшей размерности без значительных потерь в качестве. К сожалению, на эти вопросы статья не отвечает.</p>
<p>А ещё мне кажется, что нужно подумать в сторону <strong>тензорных разложений</strong>!</p>
<section class="footnotes"><hr>
<ol>
<li id="fn1"><p>Бесконечное количество контекстов не ломает рассуждений, но заставляет нас заменить инструмент линейной алгебры на функциональный анализ.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Интересно было бы посмотреть на ранги каких-нибудь несильно сложных грамматик, а потом попробовать их выучить.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Количество использованных компонент смеси тщательно скрыто в самом конце статьи, в аппендиксе Б, и составляет 15.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Тут можно возразить, что у нас уже есть нейросеть под эмбеддингами, но важно понимать, что та нейросеть только генерирует эти векторные представления, а комбинируются они всё-таки линейно. Это как если бы вы использовали линейную регрессию, <em>веса</em> которой генерировались бы нейросетью. Повысило ли бы это выразительную способность модели?<a href="#fnref4" class="footnote-back">↩</a></p></li>
</ol></section>
</div>

    <aside class="post-meta"><a class="post-author" href="../../authors/artiom-sobolev/">Артём Соболев</a>
    <time class="post-date published dt-published" datetime="2017-11-14T00:33:03+03:00" itemprop="datePublished" title="14 ноября 2017">14 ноября 2017</time></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
        <footer id="footer"><p>Contents © 2018         <a href="mailto:info@bayesgroup.ru">BayesGroup</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer><script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><!-- fancy dates --><script>
    moment.locale("ru");
    fancydates(2, "DD.MM.YYYY");
    </script><!-- end fancy dates -->
</body>
</html>
