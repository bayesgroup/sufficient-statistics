<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " vocab="http://ogp.me/ns" lang="ru">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Augment and Reduce: Stochastic Inference for Large Categorical Distributions | Codename 'Sufficient Statistics'</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/lanyon.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="canonical" href="http://bayesgroup.github.io/sufficient-statistics/posts/augment-and-reduce-stochastic-inference-for-large-categorical-distributions/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Артём Соболев">
<link rel="prev" href="../bruno-a-deep-recurrent-model-for-exchangeable-data/" title="BRUNO: A Deep Recurrent Model for Exchangeable Data" type="text/html">
<meta property="og:site_name" content="Codename 'Sufficient Statistics'">
<meta property="og:title" content="Augment and Reduce: Stochastic Inference for Large Categorical Distrib">
<meta property="og:url" content="http://bayesgroup.github.io/sufficient-statistics/posts/augment-and-reduce-stochastic-inference-for-large-categorical-distributions/">
<meta property="og:description" content="Мотивация
Типичный способ работы с категориальными величинами заключается в использовании оператора softmax, принимающего K-мерный вектор логитов, и выдающий распределение на K-мерном вероятностном си">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-09-09T10:09:00+03:00">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="variational inference">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <div class="sidebar-item">
            <p>Обзоры не являются официальной позицией группы и предназначены исключительно для внутреннего использования.</p>
        </div>
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../authors/">Авторы</a>
        <a class="sidebar-nav-item" href="../../archive.html">Архив</a>
        <a class="sidebar-nav-item" href="../../categories/">Тэги</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="../../" title="Codename 'Sufficient Statistics'" rel="home">Codename 'Sufficient Statistics'</a>
    </h3>


        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><div class="paper-link">
        <a href="https://arxiv.org/abs/1802.04220">Paper</a>
    </div>
    <header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">Augment and Reduce: Stochastic Inference for Large Categorical Distributions</a></h1>

        
        <ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/variational-inference/" rel="tag">variational inference</a></li>
        </ul></header><div class="e-content entry-content" itemprop="articleBody text">
    <h1 id="мотивация">Мотивация</h1>
<p>Типичный способ работы с категориальными величинами заключается в использовании оператора softmax, принимающего K-мерный вектор логитов, и выдающий распределение на K-мерном вероятностном симплексе.</p>
<p><span class="math display">\[
\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_{k=1}^K \exp(x_k)}
\]</span></p>
<p>Однако, если категорий много и K велико, постоянно перевычислять знаменатель становится вычислительно затратно. И даже если нас не интересуют не сами вероятности, а только их градиенты, то сложность их вычисления тоже <span class="math inline">\(O(K)\)</span>. Существует миллион статей, в которых предлагаются разные способы приближения, но почти все из них не очень обоснованные. В этой статье авторы решают эту задачу с помощью применения вариационного вывода, тем самым получая нижнюю оценку на максимизируемый логарифм правдоподобия.</p>
<h1 id="математика">Математика</h1>
<p>Основная идея статьи заключается в том, чтобы вообразить себе категориальное распределение как маргинализацию некоторого совместного распределения (с некоторыми приятными свойствами, которые бы позволили нам пользоваться <em>несмещёнными</em> стох. оценками сумм) с латентной переменной <span class="math inline">\(\varepsilon\)</span>, а потом сделать вар. вывод на неё.</p>
<p>Такое совместное авторы получают с помощью т.н. аугментации, т.е. рассмотрения дискретных случайных величин как некоторого дискретизующего преобразования над непрерывными. В частности, если у нас есть случайная величина <span class="math inline">\(y \sim \text{Categorical}(\pi_1, \dots, \pi_K)\)</span> с логитами <span class="math inline">\(\psi = (\log \pi_1, \dots, \log \pi_K)\)</span>, то</p>
<p><span class="math display">\[
y \stackrel{d}{=} \text{argmax}(\psi_k + \varepsilon_k), \quad\quad \varepsilon_k \sim \text{Gumbel}(0, 1)
\]</span></p>
<p>Где аргмаксимум берётся по индексам k, а <span class="math inline">\(\varepsilon_k\)</span> семплируются независимо и одинаково распределённо. Оказывается, что шум можно брать не только гумбелевский, натурально и другие распределения <span class="math inline">\(\varepsilon\)</span> будут давать какое-то дискретное распределение на <span class="math inline">\(y\)</span>, просто Гумбелевский даст вероятностями <span class="math inline">\(y\)</span> софтмакс над логитами <span class="math inline">\(\psi\)</span>, а другие распределения будут давать другие преобразования логитов (не всегда вычислимые аналитически).</p>
<p>Теперь можно представить <span class="math inline">\(p(y = k)\)</span> в виде маргинализации, а именно</p>
<p><span class="math display">\[
\begin{align*}
p(y = k)
&amp;= \mathbb{P}(\psi_k + \varepsilon_k &gt; \psi_j + \varepsilon_j, \forall j \not= k) \\
&amp;= \int_\mathbb{R} p(\varepsilon_k) \prod_{j\not=k} \mathbb{P}(\varepsilon_j &lt; \psi_k - \psi_j + \varepsilon_k) d\varepsilon_k \\
&amp;= \int_\mathbb{R} p(\varepsilon_k) \prod_{j\not=k} F(\psi_k - \psi_j + \varepsilon_k) d\varepsilon_k
\end{align*}
\]</span></p>
<p>Где <span class="math inline">\(p(\varepsilon)\)</span> – функция плотности нашего шума <span class="math inline">\(\varepsilon\)</span>, а <span class="math inline">\(F\)</span> – (кумулятивная) функция распределения (которая у какого-нибудь Гумбеля считается аналитически). Получается, что искомое нами совместное распределение есть</p>
<p><span class="math display">\[
p(y = k, \varepsilon_k) = p(\varepsilon_k) \prod_{j\not=k} F(\psi_k - \psi_j + \varepsilon_k)
\]</span></p>
<p>У этой на первый взгляд непримичательной плотности есть одно интересное свойство: в её логарифме произведение превратится в сумму, которую мы можем несмещённо стохастически оценивать с помощью Монте Карло. В общем, давайте делать вар. вывод на <span class="math inline">\(\varepsilon_k\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
\log p(y = k)
&amp;\ge \mathbb{E}_{q(\varepsilon_k)} \left[\log p(y = k, \varepsilon) - \log q(\varepsilon)\right] \\
&amp;= \mathbb{E}_{q(\varepsilon_k)} \left[\log p(\varepsilon_k) + \sum_{j\not=k} \log F(\psi_k - \psi_j + \varepsilon_k) - \log q(\varepsilon)\right]
\end{align*}
\]</span></p>
<p>Т.е. мы для каждого слагаемого вида <span class="math inline">\(\log p(y_n = k)\)</span> в правдоподобии заводим <span class="math inline">\(q(\varepsilon_{nk})\)</span> <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> и заменяем это слагаемое на нижнюю оценку. Однако, её наивное вычисление по-прежнему требует <span class="math inline">\(O(K)\)</span> вычислений (из-за второго слагаемого) и ничем не помогает в нашем деле. Ну, исправляется это просто, мы просто заменим сумму на выборочную сумму с масштабированием:</p>
<p><span class="math display">\[
\mathbb{E}_{S} \left[\tfrac{K-1}{|S|} \sum_{i \in S} \log F(\psi_k - \psi_i + \varepsilon_k)\right] = \sum_{j\not=k} \log F(\psi_k - \psi_j + \varepsilon_k)
\]</span></p>
<p>Где S – равномерно засемплированная выборка из <span class="math inline">\(\{1, \dots, K\} \backslash \{k\}\)</span>. Итого, заменяя второе слагаемое в нижней оценке на такую оценку, получаем выражение, сложность вычисления которой уже не <span class="math inline">\(O(K)\)</span>, а <span class="math inline">\(O(|S|)\)</span>. Ну а дальше можно делать обычный градиентный спуск, применив репараметризацию к <span class="math inline">\(q(\varepsilon_k)\)</span>.</p>
<h2 id="гумбелевский-случай">Гумбелевский случай</h2>
<p>Удивительным образом, в случае распределения Гумбеля мы можем найти оптимальное <span class="math inline">\(q^\star(\varepsilon_k)\)</span> аналитически:</p>
<p><span class="math display">\[
q^\star(\varepsilon_k) = \text{Gumbel}(\varepsilon_k; \log \eta_k^\star, 1)
\]</span></p>
<p>Где <span class="math inline">\(\eta_k^\star = 1 + \sum_{j\not=k} \exp(\psi_j - \psi_k)\)</span>, но лучше от этого не становится, т.к. сложность вычисления такого слагаемого опять же линейна. Поэтому мы будем учить приближение <span class="math inline">\(q(\varepsilon_k) = \text{Gumbel}(\varepsilon_k; \log \eta_k, 1)\)</span> градиентной оптимизацией по <span class="math inline">\(\eta_k\)</span>. Наша нижняя оценка превращается в <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p><span class="math display">\[
\mathcal{L} = 1 - \log \eta_k - \frac{1}{\eta_k} \left(1 + \sum_{j\not=k} \exp(\psi_j - \psi_k) \right)
\]</span></p>
<p>К нашему огромному успеху <span class="math inline">\(q(\varepsilon_k)\)</span> лежит в экспоненциальном классе распределений, а это позволяет нам использовать натуральный градиентный спуск из фреймворка стохастического вариационного вывода, что позволяет нам пользоваться стохастически оценками натурального параметра <span class="math inline">\(\eta_k\)</span>. Градиентное обновление тогда имеет вид</p>
<p><span class="math display">\[
\eta_k^{(t+1)} = (1 - \alpha_t) \eta_k^{(t)} + \alpha_t \tilde{\eta_k}
\]</span></p>
<p>Где <span class="math inline">\(\tilde{\eta_k} = 1 + \tfrac{K-1}{|S|} \sum_{i \in S} \exp(\psi_i - \psi_k)\)</span> – стохастическая оценка натурального параметра по выборке <span class="math inline">\(S\)</span>, а <span class="math inline">\(\alpha_t\)</span> – размер шага градиентного спуска, удовлетворяющий условиям Роббинса-Монро.</p>
<p>Итого алгоритм получается такой: <img src="../../post-images/augment-and-reduce-stochastic-inference-for-large-categorical-distributions/alg.png" class="medium"></p>
<h1 id="эксперименты">Эксперименты</h1>
<p>Авторы в основном сравниваются с другим методом (за авторством Титсиаса, являющего соавтором и этой статьи) – “<a href="http://papers.nips.cc/paper/6468-one-vs-each-approximation-to-softmax-for-scalable-estimation-of-probabilities.pdf">One-vs-Each Approximation to Softmax for Scalable Estimation of Probabilities</a>”, чуть ли не единственным так же дающим нижнюю оценку. Авторы утверждают, что у их метода зазор между настоящим правдоподобием и нижней оценкой меньше, поэтому он предпочтительнее.</p>
<p><img src="../../post-images/augment-and-reduce-stochastic-inference-for-large-categorical-distributions/fig2.png" class="medium"></p>
<p>В экспериментах они бьют OvE:</p>
<p><img src="../../post-images/augment-and-reduce-stochastic-inference-for-large-categorical-distributions/table3.png"></p>
<p>К сожалению, ни в одной из двух статей ничего не говорится о том, какого ускорения можно ожидать в сравнении с полным софтмаксом.</p>
<h1 id="резюме">Резюме</h1>
<p>Довольно интересная статья, можно использовать в качестве примера в курсе по БММО. К минусам можно отнести необходимость хранить параметры распределения <span class="math inline">\(q(\varepsilon_k)\)</span> для каждого из <span class="math inline">\(N\)</span> наблюдений в обучающей выборке.</p>
<p>Думаю, здесь можно категориальное распределение считать частным случаем марковской сети.</p>
<section class="footnotes"><hr>
<ol>
<li id="fn1"><p>Это не означает, что у нас <span class="math inline">\(NK\)</span> распределений. На самом деле, нам для каждого <span class="math inline">\(y_n\)</span> интересно только <span class="math inline">\(p(y_n=k)\)</span>, поэтому и <span class="math inline">\(\epsilon\)</span> интересен только <span class="math inline">\(k\)</span>-ый, так что их скорее <span class="math inline">\(O(N)\)</span>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Примечательно, что эта оценка уже известна человечеству лет 10 как. Когда-то её выводили, пользуясь лог-вогнутостью.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol></section>
</div>

    <aside class="post-meta"><a class="post-author" href="../../authors/artiom-sobolev/">Артём Соболев</a>
    <time class="post-date published dt-published" datetime="2018-09-09T10:09:00+03:00" itemprop="datePublished" title="09 сентября 2018">09 сентября 2018</time></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
        <footer id="footer"><p>Contents © 2018         <a href="mailto:info@bayesgroup.ru">BayesGroup</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer><script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><!-- fancy dates --><script>
    moment.locale("ru");
    fancydates(2, "DD.MM.YYYY");
    </script><!-- end fancy dates -->
</body>
</html>
