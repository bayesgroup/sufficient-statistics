<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " vocab="http://ogp.me/ns" lang="ru">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Riemannian approach to Batch Normalization | Codename 'Sufficient Statistics'</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/lanyon.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="canonical" href="http://bayesgroup.github.io/sufficient-statistics/posts/riemannian-approach-to-batch-normalization/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Андрей Атанов">
<link rel="prev" href="../wasserstein-generative-adversarial-networks/" title="Wasserstein Generative Adversarial Networks (basic and improved)" type="text/html">
<meta property="og:site_name" content="Codename 'Sufficient Statistics'">
<meta property="og:title" content="Riemannian approach to Batch Normalization">
<meta property="og:url" content="http://bayesgroup.github.io/sufficient-statistics/posts/riemannian-approach-to-batch-normalization/">
<meta property="og:description" content="Мотивация
Техника Batch Normalization стала обычной составной частью DNN. Она помогает избавиться от т.н. internal covariate shift, и приводит к более стабильной и быстрой сходимости сетей с большим ч">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-10-15T21:07:14+03:00">
<meta property="article:tag" content="batch normalization">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="mathjax">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <div class="sidebar-item">
            <p>Обзоры не являются официальной позицией группы и предназначены исключительно для внутреннего использования.</p>
        </div>
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../authors/">Авторы</a>
        <a class="sidebar-nav-item" href="../../archive.html">Архив</a>
        <a class="sidebar-nav-item" href="../../categories/">Тэги</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="../../" title="Codename 'Sufficient Statistics'" rel="home">Codename 'Sufficient Statistics'</a>
    </h3>


        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><div class="paper-link">
        <a href="https://arxiv.org/abs/1709.09603">Paper</a>
    </div>
    <header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">Riemannian approach to Batch Normalization</a></h1>

        
        <ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/batch-normalization/" rel="tag">batch normalization</a></li>
            <li><a class="tag p-category" href="../../categories/deep-learning/" rel="tag">deep learning</a></li>
        </ul></header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<h2>Мотивация</h2>
<p>Техника Batch Normalization стала обычной составной частью DNN. Она помогает избавиться от т.н. internal covariate shift, и приводит к более стабильной и быстрой сходимости сетей с большим числом слоев. Автор оригинальной статьи говорит, что это достигается за счет инвариантности прямого и обратного проходов от линейного преобразования весов, что позволяет использовать бОльший learning rate.
Пусть $x$ – активации предыдущего слоя, $w$ – вектор весов, $z = w^Tx$, тогда это свойство можно записать следующим образом:</p>
<p>$$ \text{BN}(z) = \dfrac{z - \mathbb{E}[z]}{\sqrt{\text{Var}[z]}} =  \dfrac{w^T(x - \mathbb{E}[x])}{\sqrt{w^T R_{xx} w}} = \dfrac{u^T(x - \mathbb{E}[x])}{\sqrt{u^T R_{xx} u}}$$</p>
<p>где $u = w/\|w\|, \, R_{xx} = \text{Var}[x]$</p>
<p>Авторы рассматриваемой статьи говорят, что такое свойство может в то же время приводить к проблемам за счет существования бесконечного количества конфигураций сети, которые одинаковы с точки зрения forward pass, но имеют разную норму параметров, что может привести к сильной чувствительности к параметру регуляризации. </p>
<h2>Идея</h2>
<p>Давайте решать условню задачу оптимизации с $\|w\| = \text{const}$. Но это сложно, поэтому давайте перейдем в пространство, где веса, отличающиеся домножением на константу, представлены одной точкой.</p>
<h2>Математика</h2>
<p>В качестве такого пространства авторы предлагают использовать римановское многообразие – Grassmann manifold $G(p, n), \, p &lt; n$.  $G(p, n)$ – множество всех линейных подпространств $\mathbb{R}^n$ размерности $p$. Если теперь взять многообразие $G(1, n)$ то точка на нем будет соответствовать $\{ \alpha w \, \big| \, \alpha \in \mathbb{R} \}$ для некоторого $w \in \mathbb{R}^n$, а это ровно то, чего и хотелось. В качестве представления точки на таком многообразии предлагается брать вектор $w$ с единичной нормой. Легко представлять себе $G(1, n)$ как единичную сферу. </p>
<p>Чтобы теперь запустить SGD на таком многообразии нужно много теории римановских многобразий. По факту обычный градиент $g$ (посчитанный в $\mathbb{R}^n$) проектируется на касательную плоскость к точке в которой градиент считается ($y$) и получется градиент на многобразии $h$. Затем по геодезической из точки $y$ в направлении $h$ необходимо пройти длину $\|h\|$ и попасть в новую точку $y_1$.</p>
<p><img alt="grad" src="../../post-images/riemannian-approach-to-batch-normalization/grad1.png"></p>
<p>Из данного алгоритма авторы также делают SGD c Momentum и ADAM.</p>
<h2>Регуляризация</h2>
<p>За чередой запутывающих незнакомого с римановскими многообразиями читателя формул с отсылками к книжкам с объяснениями следует небольшой раздел Regularization using variational inference, который по-началу тоже норовит блеснуть своей математичностью. По факту авторы говорят, что к предложенному методу неприменима довольно распростроненная L2 регуляризация (и правда –- $\|w\| = 1$ всегда). Поэтому они предлагают (правда они?) довольно интересную регуляризацию, которая в итоге представляет следующий лосс:</p>
<p>$$ \dfrac{\alpha}{2} \|W^T W - I\|^2_F = \dfrac{\alpha}{2} \sum_{i \neq j} \langle w_i, w_j \rangle^2 $$</p>
<p>Минимум такого функционала достигается, когда все столбцы ортогональны друг другу. Таким образом запрещается сильная зависимость между столбцами матрицы. </p>
<h2>Эксперименты</h2>
<p>Авторы сравнивали свой подход с обычным batch normalization в аспекте скорости сходимости и качестве на валидационный выборке. Для этого они взяли несколько популярных архитектур и заменили стандартные слои батчнорма своей реализацией. </p>
<p><img alt="" src="../../post-images/riemannian-approach-to-batch-normalization/speed.png"></p>
<p>Про скорость сходимости они пишут, что для обучения с обычным батчнормом лосс уменьшается только при понижении learning rate, в то время как их имплементация уменьшает лосс постепенно. Неясно, что им мешало для обычных сетей уменьшать рейт линейно или сделать exponetial decay и сравнить. Тем более, что предложенная процедура оптимизации медленнее обычного SGD или ADAM в 2.5-3.5 раз, как пишут авторы.</p>
<p>Применив свою имплементацию и оптимизацию они смогли улучшить accuracy score для ряда известных архитектур, хоть и на немного</p>
<p><img alt="" src="../../post-images/riemannian-approach-to-batch-normalization/table1.png"></p>
<p><img alt="" src="../../post-images/riemannian-approach-to-batch-normalization/table3.png"></p>
<p>В заключении авторы говорят, что данную технику можно применить также и для других техник нормализации – weight normalization, normalization propagation и layer normalization.</p>
<h2>Резюме</h2>
<p>Интересная, хоть и не новая, техника сведения задачи условной оптимизации к безусловной за счет использования римановских многообразий. Кроме того интересное и проработанное расширение обычного римановского SGD до популярных Momentum и ADAM и интересная регуляризация.</p>
</div>
    </div>

    <aside class="post-meta"><a class="post-author" href="../../authors/andrei-atanov/">Андрей Атанов</a>
    <time class="post-date published dt-published" datetime="2017-10-15T21:07:14+03:00" itemprop="datePublished" title="15 октября 2017">15 октября 2017</time></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
        <footer id="footer"><p>Contents © 2017         <a href="mailto:info@bayesgroup.ru">BayesGroup</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer><script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><!-- fancy dates --><script>
    moment.locale("ru");
    fancydates(2, "DD.MM.YYYY");
    </script><!-- end fancy dates -->
</body>
</html>
