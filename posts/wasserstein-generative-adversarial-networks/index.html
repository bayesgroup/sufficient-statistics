<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " vocab="http://ogp.me/ns" lang="ru">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Wasserstein Generative Adversarial Networks (basic and improved) | Codename 'Sufficient Statistics'</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/lanyon.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="canonical" href="http://bayesgroup.github.io/sufficient-statistics/posts/wasserstein-generative-adversarial-networks/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Алексей Умнов">
<link rel="prev" href="../improved-variational-autoencoders-for-text-modeling-using-dilated-convolutions/" title="Improved Variational Autoencoders for Text Modeling using Dilated Convolutions" type="text/html">
<link rel="next" href="../riemannian-approach-to-batch-normalization/" title="Riemannian approach to Batch Normalization" type="text/html">
<meta property="og:site_name" content="Codename 'Sufficient Statistics'">
<meta property="og:title" content="Wasserstein Generative Adversarial Networks (basic and improved)">
<meta property="og:url" content="http://bayesgroup.github.io/sufficient-statistics/posts/wasserstein-generative-adversarial-networks/">
<meta property="og:description" content="Мотивация
У GAN-ов есть много разных проблем, среди которых: 1) высокая чувствительность к архитектурам, 2) необходимость использования множества “хаков” для того, чтобы получить хорошие результаты. Н">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-10-10T21:04:57+03:00">
<meta property="article:tag" content="adversarial networks">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="generative models">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="optimal transport">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <div class="sidebar-item">
            <p>Обзоры не являются официальной позицией группы и предназначены исключительно для внутреннего использования.</p>
        </div>
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../authors/">Авторы</a>
        <a class="sidebar-nav-item" href="../../archive.html">Архив</a>
        <a class="sidebar-nav-item" href="../../categories/">Тэги</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="../../" title="Codename 'Sufficient Statistics'" rel="home">Codename 'Sufficient Statistics'</a>
    </h3>


        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><div class="paper-link">
        <a href="https://arxiv.org/abs/1701.07875">Paper</a>
    </div>
    <header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">Wasserstein Generative Adversarial Networks (basic and improved)</a></h1>

        
        <ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/adversarial-networks/" rel="tag">adversarial networks</a></li>
            <li><a class="tag p-category" href="../../categories/deep-learning/" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../categories/generative-models/" rel="tag">generative models</a></li>
            <li><a class="tag p-category" href="../../categories/optimal-transport/" rel="tag">optimal transport</a></li>
        </ul></header><div class="e-content entry-content" itemprop="articleBody text">
    <h1 id="мотивация">Мотивация</h1>
<p>У GAN-ов есть много разных проблем, среди которых: 1) высокая чувствительность к архитектурам, 2) необходимость использования множества “хаков” для того, чтобы получить хорошие результаты. На GAN-ы можно смотреть со следующей точки зрения: есть два распределения — настоящие примеры и сгенерированные, — и дискриминатор пытается измерять некоторое расстояние между этими распределениями (см. далее), а генератор пытается это расстояние минимизировать. Авторы считают, что в проблемах выше виноват выбор расстояния, и предлагают другое.</p>
<h1 id="математика">Математика</h1>
<h2 id="расстояния-между-распределениями">Расстояния между распределениями</h2>
<p>Рассмотрим класс GAN-ов, который называется <span class="math inline">\(f\)</span>-GAN-ы, туда попадают и vanilla GAN (самый первый), и DCGAN, и Improved GAN. Можно показать (см. <a href="http://papers.nips.cc/paper/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization">статью про f-GAN-ы</a>), что их генератор на самом деле пытается минимизировать <span class="math inline">\(f\)</span>-дивергенцию между реальными данными и сгенерированными им примерами, а дискриминатор вычисляет эту <span class="math inline">\(f\)</span>-дивергенцию. <span class="math inline">\(f\)</span>-дивергенцией между распределениями <span class="math inline">\(P, Q\)</span> над одним доменом <span class="math inline">\(\mathcal X\)</span> с плотностями <span class="math inline">\(p, q\)</span> называется такая штука:</p>
<p><span class="math display">\[
    D_f(P, Q) = \int_{\mathcal X} f \left(\frac{p(x)}{q(x)}
        \right) q(x) dx,
\]</span></p>
<p>где <span class="math inline">\(f\)</span> - выпуклая функция, и <span class="math inline">\(f(1) = 0\)</span>. Например, при <span class="math inline">\(f(t) = t \log t\)</span> получается KL-дивергенция.</p>
<p>У <span class="math inline">\(f\)</span>-дивергенций есть много хороших свойств, но проблема для данной задачи в том, что они плохо вычисляют расстояние между вырожденными распределениями. Действительно, если представить, что каждое из <span class="math inline">\(P\)</span> и <span class="math inline">\(Q\)</span> имеют ненулевую плотность только на какой-то поверхности небольшой размерности (а есть гипотеза, что так и есть для реальных данных), и у этих поверхностей нет пересечения, то <span class="math inline">\(f\)</span>-дивергенция будет принимать максимальное значение вне зависимости от положения поверхностей в пространстве. Отсюда всевозможная нестабильность, исчезающие градиенты и т.п.</p>
<p>Авторы предлагают решить проблему с помощью использования расстояния Wasserstein-1 distance (оно же Earth-Mover distance), которое определяется так:</p>
<p><span class="math display">\[
    W(P, Q) = \inf_{\gamma \in \prod(P, Q)} \mathbb E_{(x, y) \sim \gamma}
        \left[ d(x, y) \right],
\]</span></p>
<p>где</p>
<ul>
<li>
<span class="math inline">\(\prod(P, Q)\)</span> - множество распределений <span class="math inline">\(\gamma(x, y)\)</span>, у которых маргинальными распределениями являются <span class="math inline">\(P(x)\)</span> и <span class="math inline">\(Q(y)\)</span> соответственно.</li>
<li>
<span class="math inline">\(d(x, y)\)</span> - произвольная метрика (обычно берут L2).</li>
</ul>
<p>Интуитивно, <span class="math inline">\(W(P, Q)\)</span> рассматривает всевозможные “планы транспортировки” вероятностной массы из одного распределения в другое (<span class="math inline">\(\gamma\)</span>), меряет для каждого такого плана среднее расстояние транспортировки, и берет минимальное. Такое расстояние гораздо лучше работает с вырожденными распределениями, измеряя как раз “расстояние” между их поверхностями.</p>
<h2 id="обучение-wgan">Обучение WGAN</h2>
<p>Далее возникает вопрос - как это расстояние вычислять? Оказывается, есть теорема Канторовича-Рубинштейна, которая говорит, что у <span class="math inline">\(W(P, Q)\)</span> есть альтернативная форма (не надо пытаться понять интуитивно, (но если очень хочется, то смотри <a href="https://vincentherrmann.github.io/blog/wasserstein/">сюда</a> – прим. А.С.)):</p>
<p><span class="math display">\[
W(P, Q) = \max_{\| f \|_L \leqslant 1}
    \mathbb E_{x \sim P} [f(x)] - \mathbb E_{x \sim Q} [f(x)],
\]</span></p>
<p>где <span class="math inline">\(\| f \|_L\)</span> - это Липшицева константа для <span class="math inline">\(f\)</span>. С помощью этой формулы можно вычислять <span class="math inline">\(W(P, Q)\)</span>: взять в качестве <span class="math inline">\(f\)</span> нейронную сеть, и искать параметры градиентным подъемом по <span class="math inline">\(f\)</span>, вычисляя вместо матожидания среднее по мини-батчам (достаточно уметь сэмплировать из <span class="math inline">\(P\)</span> и <span class="math inline">\(Q\)</span>). Однако возникает вопрос контроля условия <span class="math inline">\(\| f \|_L \leqslant 1\)</span>. В исходной статье про WGAN это предлагалось делать с помощью грубой обрезки весов для <span class="math inline">\(f\)</span>, однако, как выяснилось, это очень плохая идея (параметр величины обрезки тяжело подобрать, и эта обрезка иногда может приводить к странному поведению). В последующей статье (<a href="https://arxiv.org/abs/1704.00028">Improved WGAN</a>) авторы предлагают (с некоторым теоретическим обоснованием) использовать вместо этого произвольные сети, но добавлять к оптимизируемой функции дополнительный градиентный штраф (вес которого уже гораздо легче подобрать):</p>
<p><span class="math display">\[
    W^\prime(P, Q) = \max_f \left(
        \mathbb E_{x \sim P} [f(x)] - \mathbb E_{x \sim Q} [f(x)] -
        \lambda \mathbb E_{x \sim M_{P,Q}} \left[ (\| \nabla_x f(x)\|_2 - 1)^2 \right] \right),
\]</span></p>
<p>где <span class="math inline">\(M_{P,Q}\)</span> задается как смесь: <span class="math inline">\(\hat x \sim M_{P,Q} \Leftrightarrow x \sim P\)</span>, <span class="math inline">\(y \sim Q\)</span>, <span class="math inline">\(\varepsilon \sim U[0, 1]\)</span>, <span class="math inline">\(\hat x = \varepsilon x + (1 - \varepsilon) y\)</span>. Сэмплировать из этого распределения легко, но есть тонкий момент — в этой функции оптимизируется градиент, поэтому используемая библиотека здесь должна будет вычислять вторую производную.</p>
<p>В итоге обучение WGAN выглядит так. Задаем генератор <span class="math inline">\(G\)</span> и дискриминатор <span class="math inline">\(D\)</span> как нейронные сети. Фейковые данные генерируем так: <span class="math inline">\(x_{fake} \sim P_{fake} \Leftrightarrow z \sim \mathcal{N}(0, I),\; x_{fake} = G(z)\)</span>. Реальные данные сэмплируем из обучающей выборки <span class="math inline">\(x_{real} \sim P_{real}\)</span>. Далее обучаем <span class="math inline">\(D, G\)</span> с чередующейся оптимизацией SGD для таких двух целевых функций:</p>
<p><span class="math display">\[
    \min_D \left(
        \mathbb E_{x \sim P_{fake}} [D(x)] - \mathbb E_{x \sim P_{real}} [D(x)] +
        \lambda \mathbb E_{x \sim M_{P_{real},P_{fake}}} \left[ (\| \nabla_x D(x)\|_2 - 1)^2 \right] \right);
\]</span> <span class="math display">\[
    \min_G \left( - \mathbb E_{z \sim \mathcal{N}(0, I)} [D(G(x))] \right).
\]</span></p>
<h1 id="эксперименты">Эксперименты</h1>
<p>Статья про генеративные модели, поэтому в статьях куча примеров обучения на разных данных. Картиночки ниже, а пока список основных свойств, которые авторы демонстрируют:</p>
<ul>
<li>WGAN более устойчив к выбору архитектур, чем f-GAN. Можно варьировать разные детали (convolutional/fully-connected, нормализация, и т.п.) без сильной потери качества. f-GAN при аналогичных вариациях обычно полностью перестает работать.</li>
<li>Можно подобрать архитектуры, которые для WGAN дают state-of-the-art-качество.</li>
<li>В процессе обучения можно использовать Wasserstein distance как внутреннюю метрику качества модели. В отличие от качества дискриминатора в f-GAN (которое часто колеблется в районе какого-то значения), Wasserstein distance плавно убывает в процессе обучения.</li>
</ul>
<h1 id="резюме">Резюме</h1>
<p>Интересная идея, которая, во-первых, дает уже достаточно работоспособный и удобный GAN, и во-вторых, которую можно дальше развивать.</p>
<h1 id="примеры">Примеры</h1>
<p>LSUN bedrooms 128x128</p>
<p><img src="../../post-images/wasserstein-generative-adversarial-networks/lsun.png"></p>
<p>CIFAR-10 32x32</p>
<p><img src="../../post-images/wasserstein-generative-adversarial-networks/cifar10.png"></p>
<p>Character-level sentence generation (Billion Word dataset, 32-character sentences)</p>
<p><img src="../../post-images/wasserstein-generative-adversarial-networks/text.png"></p>
    </div>

    <aside class="post-meta"><a class="post-author" href="../../authors/aleksei-umnov/">Алексей Умнов</a>
    <time class="post-date published dt-published" datetime="2017-10-10T21:04:57+03:00" itemprop="datePublished" title="10 октября 2017">10 октября 2017</time></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
        <footer id="footer"><p>Contents © 2017         <a href="mailto:info@bayesgroup.ru">BayesGroup</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer><script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><!-- fancy dates --><script>
    moment.locale("ru");
    fancydates(2, "DD.MM.YYYY");
    </script><!-- end fancy dates -->
</body>
</html>
