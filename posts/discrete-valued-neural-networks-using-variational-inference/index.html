<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " vocab="http://ogp.me/ns" lang="ru">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Discrete-Valued Neural Networks Using Variational Inference | Codename 'Sufficient Statistics'</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/lanyon.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="canonical" href="http://bayesgroup.github.io/sufficient-statistics/posts/discrete-valued-neural-networks-using-variational-inference/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Артём Соболев">
<link rel="prev" href="../deep-exponential-families/" title="Deep Exponential Families" type="text/html">
<link rel="next" href="../debiasing-evidence-approximations-on-importance-weighted-autoencoders-and-jackknife-variational-inference/" title="Debiasing Evidence Approximations: on Importance-Weighted Autoencoders and Jackknife Variational Inference" type="text/html">
<meta property="og:site_name" content="Codename 'Sufficient Statistics'">
<meta property="og:title" content="Discrete-Valued Neural Networks Using Variational Inference">
<meta property="og:url" content="http://bayesgroup.github.io/sufficient-statistics/posts/discrete-valued-neural-networks-using-variational-inference/">
<meta property="og:description" content="Мотивация
Хотим дискретные сети с дискретными весами (например, \(w \in \{-1, 0, +1\}\), но можно и более широкий спектр рассмотреть) и бинарными активациями. Хотим потому что такие сети могут быстрее">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-10-28T16:57:42+03:00">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="discretization">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="variational inference">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <div class="sidebar-item">
            <p>Обзоры не являются официальной позицией группы и предназначены исключительно для внутреннего использования.</p>
        </div>
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../authors/">Авторы</a>
        <a class="sidebar-nav-item" href="../../archive.html">Архив</a>
        <a class="sidebar-nav-item" href="../../categories/">Тэги</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="../../" title="Codename 'Sufficient Statistics'" rel="home">Codename 'Sufficient Statistics'</a>
    </h3>


        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><div class="paper-link">
        <a href="https://openreview.net/pdf?id=r1h2DllAW">Paper</a>
    </div>
    <header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">Discrete-Valued Neural Networks Using Variational Inference</a></h1>

        
        <ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/deep-learning/" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../categories/discretization/" rel="tag">discretization</a></li>
            <li><a class="tag p-category" href="../../categories/variational-inference/" rel="tag">variational inference</a></li>
        </ul></header><div class="e-content entry-content" itemprop="articleBody text">
    <h1 id="мотивация">Мотивация</h1>
<p>Хотим дискретные сети с дискретными весами (например, <span class="math inline">\(w \in \{-1, 0, +1\}\)</span>, но можно и более широкий спектр рассмотреть) и бинарными активациями. Хотим потому что такие сети могут быстрее работать на специализированном железе, а места занимать меньше.</p>
<h1 id="математика">Математика</h1>
<p>Делаем вар. вывод на веса:</p>
<p><span class="math display">\[
\log p(y|x) \ge
\mathbb{E}_{q(W)} \log p(y|W, x) - D_{KL}(q(W)||p(W)) \to \max_q
\]</span></p>
<p>Ожидаемое правдоподобие нельзя посчитать аналитически, т.к. эта сумма экспоненциально большого числа разных конфигураций нейросетей. Однако, можно заметить, что можно применить ЦПТ к преактивациям: обозначим нейроны <span class="math inline">\(k\)</span>-го слоя за <span class="math inline">\(x^{(k)}\)</span>, а за <span class="math inline">\(a^{(k)}\)</span> – преактивации этого слоя. То есть, <span class="math inline">\(x^{(k)} = \text{sign}(a^{(k)}) = \text{sign}(W^{(k)} x^{(k-1)})\)</span> (bias’ы “вшиты” в <span class="math inline">\(W^{(k)}\)</span>). Тогда преактивации <span class="math inline">\(a^{(k)}\)</span> есть сумма большого количества независимых случайных величин и можно понадеяться, что они будут распределены нормально. Мат. ожидание преактиваций можно посчитать аналитически:</p>
<p><span class="math display">\[
\mathbb{E} a^{(k)} = \mathbb{E} W^{(k)} x^{(k-1)} = \mathbb{E} W^{(k)} \mathbb{E} x^{(k-1)}
\]</span></p>
<p>Ковариация в общем случае будет полной матрицей (кажется, что следующую формулу должно быть просто посчитать в замкнутом виде, но я знаю, как записать формулу для результата в матричном виде) <span class="math display">\[
\mathbb{E} a^{(k)} (a^{(k)})^T = \mathbb{E} W^{(k)} x^{(k-1)} (x^{(k)})^T (W^{(k)})^T  = \mathbb{E}_W \left[W^{(k)} \mathbb{E}_x [x^{(k-1)} (x^{(k-1)})^T] (W^{(k)})^T\right]
\]</span></p>
<p>Авторы статьи тоже не захотели считать полную матрицу ковариаций, поэтому ограничились диагональным приближением, т.е. предположили, что преактивации независимы друг от друга <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Тогда второй момент <span class="math inline">\(i\)</span>-й преактивации</p>
<p><span class="math display">\[
\begin{align*}
a^{(k)}_{i}
&amp;= \mathbb{E} (W^{(k)}_i)^T x^{(k-1)} (W^{(k)}_i)^T x^{(k-1)}
= \mathbb{E} \text{Tr}\Bigl[ (W^{(k)}_i)^T x^{(k-1)} (W^{(k)}_i)^T x^{(k-1)} \Bigr] \\
&amp;= \mathbb{E} \text{Tr}\Bigl[ (W^{(k)}_i)^T x^{(k-1)} (x^{(k-1)})^T W^{(k)}_i \Bigr] \\
&amp;= \text{Tr}\Bigl[ \mathbb{E}_W \left[W^{(k)}_i (W^{(k)}_i)^T \right] \mathbb{E}_x \left[x^{(k-1)} (x^{(k-1)})^T\right] \Bigr]
\end{align*}
\]</span></p>
<p>Обе матрицы диагональны в силу независимости (предполагаемой или модельной) всех случайных величин. Можно считать, что входы имеют нулевую дисперсию.</p>
<p>После применения к преактивациям функции знака sign, наши нормальные случайные величины превращаются в нечто среднее между Радемахеровские (знаковые Бернулли) с вероятностью +1 равной <span class="math inline">\(\text{erf}\left(\tfrac{\mu}{\sqrt{2} \sigma}\right)\)</span> (без предположения о независимости преактиваций найти распределение самих активаций было бы труднее). Моменты этой величины используются при подсчёте распределения преактиваций следующего слоя.</p>
<p>Наконец, в самом конце нашей сети стоит софтмакс. Распределение его выходов аналитически посчитать не получится, поэтому авторы просто раскладывают его в ряд Тейлора до второго члена вокруг мат. ожидания преактиваций.</p>
<p>Итого, вся эта схема является своего рода приближённым аналитическим интегрированием ожидаемой реконструкции (KL считается аналитически), где мы выинтегрируем дискретность, поэтому и дифференцировать через них не надо, и дисперсия получается маленькой (мы тут нигде Монте-Карло оценки не используем, хочу заметить!)</p>
<h2 id="распределения">Распределения</h2>
<p>Пока это была общая математика, применимая к любым распределениям. А можно рассматривать конкретные, например, набор весов <span class="math inline">\(\mathbb{D} = \{-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75\}\)</span> использует всего 3 бита (и ещё даже осталось бы место на ещё одно значение, но авторам нравится симметрия). Авторы рассматривают 2 вида распределений для приближения <span class="math inline">\(q(W)\)</span>:</p>
<ol type="1">
<li>
<strong>Произвольное распределение</strong>: для каждого исхода хранится своя вероятность, например, для примера выше их было бы 7. Вероятности, конечно же, должны суммироваться в единицу, поэтому чтобы свести оптимизационную задачу с ограничениями к безусловной оптимизации мы будем оптимизировать по логитам, которые пропустим через софтмакс для получения этих суммирующихся в 1 весов. Заметим, по-сути у нас в обчных сетях каждому весу соответствует одно число – он сам – а тут столько чисел, сколько значений он может принимать <span class="math inline">\(|\mathbb{D}|\)</span>. Неприятно.</li>
<li>
<strong>Дискретизированное Нормальное</strong>: чтобы количество параметров не зависело от количества значений, нужно как-то ограничить гибкость распределения на них. Например, ограничить какое-нибудь известное распределение вроде Гауссовского на <span class="math inline">\(\mathbb{D}\)</span>. Тогда логиты (аргументы софтмакса) будут иметь вид <span class="math inline">\(-(x - \mu_w)^2 / (2 \sigma_m^2)\)</span> где <span class="math inline">\(x \in \mathbb{D}\)</span>, а <span class="math inline">\(\mu_w, \sigma_w^2\)</span> – единственные 2 параметра этого распределения.</li>
<li>
<strong>Сдвинутое Биномиальное</strong>: а ещё можно взять биномиальное распределение и сдвинуть его. Если <span class="math inline">\(\text{Binomial}(2n, p)\)</span> даёт семплы от 0 до 2n, то <span class="math inline">\(\text{Binomial}(2n, p)-n\)</span> даёт семплы от <span class="math inline">\(-n\)</span> до <span class="math inline">\(n\)</span>, например для <span class="math inline">\(n=1\)</span> можно получить тернарные веса <span class="math inline">\(\{-1, 0, +1\}\)</span>. Такой подход вообще использует только один параметр <span class="math inline">\(p\)</span> – с одной стороны это плюс по эффективности, а с другой стороны – минус по выразительности. Некоторые входы нейросети могут быть совсем неревантны и их веса хотелось бы занулять, а такое распределение этого делать не умеет. Поэтому авторы не экспериментируют с ним для первого слоя.</li>
</ol>
<p>Авторы утверждают, что как правило весам входа нужна высокая точность, а всем остальным сгодится уменьшенная. Поэтому распределение весов первого слоя стоится с помощью одноо из первых двух распределений и 7-значного представления, а веса всех остальных слоёв используются тернарные.</p>
<p>В качестве априорных были выбраны более-менее согласованные распределения. Так, для первого слоя в качестве априорного <span class="math inline">\(p(W_1)\)</span> было использовано то самое дискретизованное нормальное со среним 0 и настраиваемой дисперсией <span class="math inline">\(\gamma\)</span>, а для следующих слоёв <span class="math inline">\(\text{Binomial}(2, 0.5)\)</span></p>
<p>На обучении получается такой ансамбль, который авторы называют PFP – probabilistic Forward Pass. В нём можно приближённо посчитать мат. ожидание софтмакса (по-Байесовски, то бишь) без семплирований за счёт “знания” распределений во все моменты времени. На тесте можно делать так же (но тогда быстрее не станет), либо взять наиболее вероятную конфигурацию, т.е. вместо вероятностных весов на каждом слое используется мода <span class="math inline">\(q(W^{(k)})\)</span>.</p>
<h2 id="хаки">Хаки</h2>
<p>Для большей устойчивости обучения авторы деляет преактивации на корень из количества входных нейронов – дескать, чтобы не подбираться слишком близко к областям насыщения erf. На тесте это ни на что не влияет, ведь там важен только знак преактивции.</p>
<p>Далее у авторов начинаются проблемы слишком сильного априорного распределения, KL-дивергенция с которым (учитывая большое количество свободных параметров в сети) забарывает правдоподобие. Поэтому предлагается взять <span class="math inline">\(\lambda\)</span>-взвешенное среднее ожидаемой конструкции и KL-дивергенции с априорным, что как бы соответствует повторению обучающей выборки <span class="math inline">\(\lambda / (1-\lambda)\)</span> раз. Но делают авторы так не всегда.</p>
<p>Кроме того, авторы используют дропаут. Вроде, как обычно.</p>
<h1 id="эксперименты">Эксперименты</h1>
<p>Сравниваются на MNIST’е (и каких-то его вариантах) с непрерывными сетями (NN real, с батчнормом и дропаутом!) и какой-то ранней работой по бинаризации нейросетей с помощью Straight Through Estimator’а (NN STE). Для бейзлайнов тюнят гиперпараметры с помощью Байесовской оптимизации. Предлагаемый метод обозначается NN VI (сравнивают разные комбинации распределений на веса и PFP против конфигурации из мод).</p>
<p>Во всех случаях в нейросети было 2 скрытых слоя (размерности 1200, думаю, достаточно для ЦПТ) и один выходной (плюс входы).</p>
<p><img src="../../post-images/discrete-valued-neural-networks-using-variational-inference/table1.png"></p>
<h2 id="ансамбли">Ансамбли</h2>
<p>Окей, PFP – это хорошо, но оно не быстрее обычных сетей, да и параметров слишком много просит. Можно попробовать взять ансамбль из нескольких нейросетей и ими постараться приблизить PFP. Авторы взяли MNIST Rotated Background – самый сложный из имеющихся датасетов – посмотрели, что там происходит с качеством по мере роста ансамбля.</p>
<p><img src="../../post-images/discrete-valued-neural-networks-using-variational-inference/ensemble1.png"></p>
<p>Здесь изображена Classification Error (CE). На первом графике – обычные ансамбли, где мы просто заранее независимо насемплировали кучу моделей из <span class="math inline">\(q(W)\)</span> и смотрим на качество такого ансамбля. Получается норм, но моделей нужно слишком много. Поэтому на втором графике авторы показывают, насколько хорошо работает greedy averaging – семплируют 100 сетей из <span class="math inline">\(q(W)\)</span>, оставляют только те, что ведут к низкой ошибке <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<h2 id="разреженность">Разреженность</h2>
<p>Потом авторы берут одну лучшую модель и смотрят, как много там нулей в весах.</p>
<p><img src="../../post-images/discrete-valued-neural-networks-using-variational-inference/sparsity.png"></p>
<h1 id="резюме">Резюме</h1>
<p>Очень круто потому что я думал о чём-то похожем. Тем не менее у статьи есть немало минусов:</p>
<ul>
<li>Сделано много предположений – на практике они выполняться не будут, а будут вносить bias в градиенты.</li>
<li>Эксперименты только на разных видах MNIST’а – нет экспериментов со свёрточными сетями, а есть сомнения в том, что метод с ними вообще будет работать.</li>
<li>Нет более эффективных бейзлайнов вроде гумбель-софтмакса – это особенно важно, ведь там можно контроллировать качество приближения</li>
</ul>
<p>Надо, правда, понимать, что эта статья ещё не прошла review и её вполне могут reject’нуть. Но ценность идеи это не уменьшает.</p>
<section class="footnotes"><hr>
<ol>
<li id="fn1"><p>В статьи про SELU так уже делали, напоминаю, что это может поломать всё для свёрточных сетей, на которых авторы не тестируются.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Видимо, просто проходят по всем засемпленным нейросетям и берут только те, что уменьшают ошибку на валидации.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol></section>
</div>

    <aside class="post-meta"><a class="post-author" href="../../authors/artiom-sobolev/">Артём Соболев</a>
    <time class="post-date published dt-published" datetime="2017-10-28T16:57:42+03:00" itemprop="datePublished" title="28 октября 2017">28 октября 2017</time></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
        <footer id="footer"><p>Contents © 2017         <a href="mailto:info@bayesgroup.ru">BayesGroup</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer><script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><!-- fancy dates --><script>
    moment.locale("ru");
    fancydates(2, "DD.MM.YYYY");
    </script><!-- end fancy dates -->
</body>
</html>
